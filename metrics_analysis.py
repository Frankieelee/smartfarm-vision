"""
YOLO æ¨¡å‹æŒ‡æ ‡åˆ†æè„šæœ¬
ä¸“æ³¨äº Recall å’Œ Classification Accuracy è®¡ç®—
"""

from ultralytics import YOLO
from pathlib import Path
import pandas as pd
from datetime import datetime
import json
import numpy as np

# ============================================================
# é…ç½®åŒºåŸŸ
# ============================================================

# æ¨¡å‹é…ç½®
MODEL_PATH = '/tmp/pycharm_project_949/runs/detect/runs/train/seedTrueLeaf.v12i.yolov11_yolo11s_800_20260201_131735/weights/best.pt'

# æ•°æ®é›†é…ç½®
DATA_YAML = './datasets/seedTrueLeaf.v12i.yolov11/data.yaml'

# è¯„ä¼°å‚æ•°
EVAL_ARGS = {
    'imgsz': 640,
    'batch': 8,
    'conf': 0.25,           # å¯è°ƒæ•´ä»¥ä¼˜åŒ– Recall/Precision
    'iou': 0.5,
    'max_det': 300,
    'device': 0,
    'workers': 6,
    'save_json': True,
    'verbose': True,
}

# è¾“å‡ºé…ç½®
OUTPUT_DIR = Path('metrics_analysis')
TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')


# ============================================================
# æ ¸å¿ƒå‡½æ•°
# ============================================================

def calculate_recall_metrics(model, data_yaml, conf_threshold=0.25):
    """
    è®¡ç®— Recall ç›¸å…³æŒ‡æ ‡
    
    Recall = æ­£ç¡®æ£€æµ‹åˆ°çš„ç›®æ ‡æ•° / æ‰€æœ‰çœŸå®ç›®æ ‡æ•°
    
    Args:
        model: YOLO æ¨¡å‹å®ä¾‹
        data_yaml: æ•°æ®é›†é…ç½®æ–‡ä»¶
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        
    Returns:
        dict: åŒ…å« Recall ç›¸å…³æŒ‡æ ‡çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è®¡ç®— Recall æŒ‡æ ‡")
    print(f"{'='*60}")
    print(f"ğŸ“ æ•°æ®é›†: {data_yaml}")
    print(f"ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
    print(f"{'='*60}\n")
    
    # è¿è¡ŒéªŒè¯
    results = model.val(
        data=data_yaml,
        split='val',
        conf=conf_threshold,
        iou=EVAL_ARGS['iou'],
        max_det=EVAL_ARGS['max_det'],
        device=EVAL_ARGS['device'],
        save_json=EVAL_ARGS['save_json'],
        verbose=False,
        plots=False,
    )
    
    # æå– Recall æŒ‡æ ‡
    recall_metrics = {
        'overall_recall': float(results.box.mr),  # Mean Recall (æ‰€æœ‰ç±»åˆ«å¹³å‡)
        'precision': float(results.box.mp),       # ç”¨äºå¯¹æ¯”
        'f1_score': 0.0,
        'per_class_recall': {},
        'conf_threshold': conf_threshold,
    }
    
    # è®¡ç®— F1-Score
    if recall_metrics['precision'] > 0 and recall_metrics['overall_recall'] > 0:
        recall_metrics['f1_score'] = 2 * recall_metrics['precision'] * recall_metrics['overall_recall'] / \
                                     (recall_metrics['precision'] + recall_metrics['overall_recall'])
    
    # æ¯ä¸ªç±»åˆ«çš„ Recall
    if hasattr(results.box, 'ap_class_index') and results.box.ap_class_index is not None:
        for i, class_idx in enumerate(results.box.ap_class_index):
            class_name = model.names[int(class_idx)]
            # Recall per class (ä» results.box.r è·å–)
            if hasattr(results.box, 'r') and i < len(results.box.r):
                class_recall = float(results.box.r[i])
            else:
                class_recall = 0.0
            
            recall_metrics['per_class_recall'][class_name] = {
                'recall': class_recall,
                'precision': float(results.box.p[i]) if i < len(results.box.p) else 0.0,
                'ap50': float(results.box.ap50[i]),
            }
    
    # æ‰“å°ç»“æœ
    print(f"âœ… Recall è®¡ç®—å®Œæˆ\n")
    print(f"{'='*60}")
    print(f"ğŸ“ˆ æ•´ä½“ Recall æŒ‡æ ‡:")
    print(f"{'='*60}")
    print(f"  Overall Recall:  {recall_metrics['overall_recall']:.1%}  {'âœ…' if recall_metrics['overall_recall'] >= 0.85 else 'âš ï¸'}")
    print(f"  Precision:       {recall_metrics['precision']:.1%}")
    print(f"  F1-Score:        {recall_metrics['f1_score']:.1%}  {'âœ…' if recall_metrics['f1_score'] >= 0.85 else 'âš ï¸'}")
    print(f"  Conf Threshold:  {conf_threshold}")
    
    if recall_metrics['per_class_recall']:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å„ç±»åˆ« Recall:")
        print(f"{'='*60}")
        print(f"  {'ç±»åˆ«':<15} {'Recall':<10} {'Precision':<12} {'AP50':<10}")
        print(f"  {'-'*50}")
        for class_name, metrics in recall_metrics['per_class_recall'].items():
            status = 'âœ…' if metrics['recall'] >= 0.85 else 'âš ï¸'
            print(f"  {class_name:<15} {metrics['recall']:.1%}     {metrics['precision']:.1%}      {metrics['ap50']:.1%}  {status}")
    
    print(f"{'='*60}\n")
    
    return recall_metrics


def calculate_classification_accuracy(model, data_yaml, conf_threshold=0.25, iou_threshold=0.5):
    """
    è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ï¼ˆClassification Accuracyï¼‰
    
    å®šä¹‰ï¼šå¯¹äºæ‰€æœ‰æ£€æµ‹åˆ°çš„æ¡†ï¼Œåˆ†ç±»æ­£ç¡®çš„æ¯”ä¾‹
    æ³¨æ„ï¼šè¿™é‡Œåªå…³å¿ƒç±»åˆ«æ˜¯å¦æ­£ç¡®ï¼Œä¸å…³å¿ƒæ¡†çš„ä½ç½®ç²¾åº¦
    
    Args:
        model: YOLO æ¨¡å‹å®ä¾‹
        data_yaml: æ•°æ®é›†é…ç½®æ–‡ä»¶
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        iou_threshold: IoU é˜ˆå€¼ï¼ˆç”¨äºåŒ¹é…é¢„æµ‹æ¡†å’ŒçœŸå®æ¡†ï¼‰
        
    Returns:
        dict: åŒ…å«åˆ†ç±»å‡†ç¡®ç‡çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡")
    print(f"{'='*60}")
    print(f"ğŸ“ æ•°æ®é›†: {data_yaml}")
    print(f"ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
    print(f"ğŸ¯ IoU é˜ˆå€¼: {iou_threshold}")
    print(f"{'='*60}\n")
    
    # è¿è¡ŒéªŒè¯å¹¶ä¿å­˜ JSON ç»“æœ
    results = model.val(
        data=data_yaml,
        split='val',
        conf=conf_threshold,
        iou=iou_threshold,
        max_det=EVAL_ARGS['max_det'],
        device=EVAL_ARGS['device'],
        save_json=True,
        verbose=False,
        plots=False,
    )
    
    # æ³¨æ„ï¼šç²¾ç¡®çš„åˆ†ç±»å‡†ç¡®ç‡éœ€è¦è§£æé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    # è¿™é‡Œä½¿ç”¨ Precision ä½œä¸ºè¿‘ä¼¼ï¼ˆæ£€æµ‹æ­£ç¡® â‰ˆ åˆ†ç±»æ­£ç¡®ï¼‰
    # å¦‚æœéœ€è¦æ›´ç²¾ç¡®çš„è®¡ç®—ï¼Œéœ€è¦ä» results ä¸­æå–è¯¦ç»†çš„é¢„æµ‹ç»“æœ
    
    classification_metrics = {
        'classification_accuracy_approx': float(results.box.mp),  # Precision ä½œä¸ºè¿‘ä¼¼å€¼
        'per_class_accuracy': {},
        'conf_threshold': conf_threshold,
        'iou_threshold': iou_threshold,
        'note': 'Classification accuracy approximated by Precision (correct detections / all detections)',
    }
    
    # æ¯ä¸ªç±»åˆ«çš„åˆ†ç±»å‡†ç¡®ç‡ï¼ˆä½¿ç”¨ Precisionï¼‰
    if hasattr(results.box, 'ap_class_index') and results.box.ap_class_index is not None:
        for i, class_idx in enumerate(results.box.ap_class_index):
            class_name = model.names[int(class_idx)]
            
            classification_metrics['per_class_accuracy'][class_name] = {
                'accuracy_approx': float(results.box.p[i]) if i < len(results.box.p) else 0.0,
                'recall': float(results.box.r[i]) if i < len(results.box.r) else 0.0,
                'ap50': float(results.box.ap50[i]),
            }
    
    # æ‰“å°ç»“æœ
    print(f"âœ… åˆ†ç±»å‡†ç¡®ç‡è®¡ç®—å®Œæˆ\n")
    print(f"{'='*60}")
    print(f"ğŸ“ˆ åˆ†ç±»å‡†ç¡®ç‡ (è¿‘ä¼¼):")
    print(f"{'='*60}")
    print(f"  æ•´ä½“å‡†ç¡®ç‡:  {classification_metrics['classification_accuracy_approx']:.1%}  {'âœ…' if classification_metrics['classification_accuracy_approx'] >= 0.85 else 'âš ï¸'}")
    print(f"  ")
    print(f"  ğŸ“ è¯´æ˜: ä½¿ç”¨ Precision ä½œä¸ºåˆ†ç±»å‡†ç¡®ç‡çš„è¿‘ä¼¼å€¼")
    print(f"         (æ­£ç¡®æ£€æµ‹æ•° / æ‰€æœ‰æ£€æµ‹æ•°)")
    
    if classification_metrics['per_class_accuracy']:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å„ç±»åˆ«åˆ†ç±»å‡†ç¡®ç‡:")
        print(f"{'='*60}")
        print(f"  {'ç±»åˆ«':<15} {'å‡†ç¡®ç‡':<10} {'Recall':<10} {'AP50':<10}")
        print(f"  {'-'*50}")
        for class_name, metrics in classification_metrics['per_class_accuracy'].items():
            status = 'âœ…' if metrics['accuracy_approx'] >= 0.85 else 'âš ï¸'
            print(f"  {class_name:<15} {metrics['accuracy_approx']:.1%}     {metrics['recall']:.1%}     {metrics['ap50']:.1%}  {status}")
    
    print(f"{'='*60}\n")
    
    return classification_metrics


def calculate_localization_accuracy(model, data_yaml, conf_threshold=0.25, iou_threshold=0.5):
    """
    è®¡ç®—å®šä½å‡†ç¡®ç‡ï¼ˆLocalization Accuracyï¼‰
    
    å®šä¹‰ï¼šåœ¨æ‰€æœ‰æ£€æµ‹æ¡†ä¸­ï¼ŒIoU â‰¥ iou_threshold çš„æ¡†å æ¯”ï¼ˆä¸è€ƒè™‘ç±»åˆ«æ˜¯å¦æ­£ç¡®ï¼‰
    è¿™ä¸ªæŒ‡æ ‡è¡¡é‡æ¨¡å‹"æ‰¾å¯¹ä½ç½®"çš„èƒ½åŠ›
    
    Args:
        model: YOLO æ¨¡å‹å®ä¾‹
        data_yaml: æ•°æ®é›†é…ç½®æ–‡ä»¶
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        iou_threshold: IoU é˜ˆå€¼
        
    Returns:
        dict: åŒ…å«å®šä½å‡†ç¡®ç‡çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è®¡ç®—å®šä½å‡†ç¡®ç‡")
    print(f"{'='*60}")
    print(f"ğŸ“ æ•°æ®é›†: {data_yaml}")
    print(f"ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
    print(f"ğŸ¯ IoU é˜ˆå€¼: {iou_threshold}")
    print(f"{'='*60}\n")
    
    # è¿è¡Œé¢„æµ‹
    img_dir = Path(data_yaml).parent / 'valid' / 'images'
    label_dir = Path(data_yaml).parent / 'valid' / 'labels'
    
    results = model.predict(
        source=str(img_dir),
        conf=conf_threshold,
        iou=EVAL_ARGS['iou'],
        device=EVAL_ARGS['device'],
        save=False,
        verbose=False,
    )
    
    # è·å–å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨ï¼ˆæŒ‰ç…§ predict çš„é¡ºåºï¼‰
    img_files = sorted(list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')) + 
                      list(img_dir.glob('*.jpeg')) + list(img_dir.glob('*.JPG')))
    
    # ç»Ÿè®¡å®šä½å‡†ç¡®ç‡
    total_predictions = 0
    correct_localizations = 0  # IoU â‰¥ threshold çš„æ£€æµ‹æ•°ï¼ˆä¸ç®¡ç±»åˆ«ï¼‰
    correct_detections = 0     # IoU â‰¥ threshold ä¸”ç±»åˆ«æ­£ç¡®
    
    per_class_stats = {}
    for class_name in model.names.values():
        per_class_stats[class_name] = {
            'predictions': 0,
            'correct_localizations': 0,
            'correct_detections': 0,
        }
    
    # éå†æ¯å¼ å›¾ç‰‡çš„é¢„æµ‹ç»“æœ
    for idx, result in enumerate(results):
        if result.boxes is None or len(result.boxes) == 0:
            continue
            
        pred_boxes = result.boxes.xyxy.cpu().numpy()  # é¢„æµ‹æ¡† (N, 4)
        pred_classes = result.boxes.cls.cpu().numpy().astype(int)  # é¢„æµ‹ç±»åˆ«
        pred_confs = result.boxes.conf.cpu().numpy()  # ç½®ä¿¡åº¦
        
        # è·å–å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
        if idx < len(img_files):
            img_file = img_files[idx]
            label_file = label_dir / f"{img_file.stem}.txt"
            
            # è¯»å–çœŸå®æ ‡ç­¾
            if label_file.exists():
                gt_boxes_list = []
                gt_classes_list = []
                
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            cls = int(parts[0])
                            x_center = float(parts[1])
                            y_center = float(parts[2])
                            width = float(parts[3])
                            height = float(parts[4])
                            
                            gt_classes_list.append(cls)
                            gt_boxes_list.append([x_center, y_center, width, height])
                
                if len(gt_boxes_list) > 0:
                    gt_boxes = np.array(gt_boxes_list)  # (M, 4) æ ¼å¼ï¼šxywhn
                    gt_classes = np.array(gt_classes_list, dtype=int)
                    
                    # è½¬æ¢çœŸå®æ¡†æ ¼å¼ï¼šxywhn -> xyxy
                    img_h, img_w = result.orig_shape
                    gt_boxes_xyxy = np.zeros_like(gt_boxes)
                    gt_boxes_xyxy[:, 0] = (gt_boxes[:, 0] - gt_boxes[:, 2] / 2) * img_w  # x1
                    gt_boxes_xyxy[:, 1] = (gt_boxes[:, 1] - gt_boxes[:, 3] / 2) * img_h  # y1
                    gt_boxes_xyxy[:, 2] = (gt_boxes[:, 0] + gt_boxes[:, 2] / 2) * img_w  # x2
                    gt_boxes_xyxy[:, 3] = (gt_boxes[:, 1] + gt_boxes[:, 3] / 2) * img_h  # y2
                    
                    # è®¡ç®—æ¯ä¸ªé¢„æµ‹æ¡†ä¸æ‰€æœ‰çœŸå®æ¡†çš„ IoU
                    for i, pred_box in enumerate(pred_boxes):
                        total_predictions += 1
                        pred_class = pred_classes[i]
                        class_name = model.names[pred_class]
                        per_class_stats[class_name]['predictions'] += 1
                        
                        # è®¡ç®—ä¸æ‰€æœ‰çœŸå®æ¡†çš„ IoU
                        max_iou = 0
                        matched_gt_class = -1
                        
                        for j, gt_box in enumerate(gt_boxes_xyxy):
                            iou = compute_iou(pred_box, gt_box)
                            if iou > max_iou:
                                max_iou = iou
                                matched_gt_class = gt_classes[j]
                        
                        # ç»Ÿè®¡å®šä½å‡†ç¡®ç‡ï¼ˆIoU â‰¥ thresholdï¼‰
                        if max_iou >= iou_threshold:
                            correct_localizations += 1
                            per_class_stats[class_name]['correct_localizations'] += 1
                            
                            # å¦‚æœç±»åˆ«ä¹Ÿæ­£ç¡®ï¼Œåˆ™è®¡æ•°
                            if matched_gt_class == pred_class:
                                correct_detections += 1
                                per_class_stats[class_name]['correct_detections'] += 1
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    localization_accuracy = correct_localizations / total_predictions if total_predictions > 0 else 0
    detection_accuracy = correct_detections / total_predictions if total_predictions > 0 else 0
    
    localization_metrics = {
        'localization_accuracy': localization_accuracy,  # IoU â‰¥ threshold çš„æ¯”ä¾‹
        'detection_accuracy': detection_accuracy,        # IoU â‰¥ threshold ä¸”ç±»åˆ«æ­£ç¡®çš„æ¯”ä¾‹
        'total_predictions': total_predictions,
        'correct_localizations': correct_localizations,
        'correct_detections': correct_detections,
        'conf_threshold': conf_threshold,
        'iou_threshold': iou_threshold,
        'per_class': {},
    }
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å®šä½å‡†ç¡®ç‡
    for class_name, stats in per_class_stats.items():
        if stats['predictions'] > 0:
            localization_metrics['per_class'][class_name] = {
                'localization_accuracy': stats['correct_localizations'] / stats['predictions'],
                'detection_accuracy': stats['correct_detections'] / stats['predictions'],
                'predictions': stats['predictions'],
            }
    
    # æ‰“å°ç»“æœ
    print(f"âœ… å®šä½å‡†ç¡®ç‡è®¡ç®—å®Œæˆ\n")
    print(f"{'='*60}")
    print(f"ğŸ“ˆ å®šä½å‡†ç¡®ç‡:")
    print(f"{'='*60}")
    print(f"  æ€»é¢„æµ‹æ¡†æ•°:      {total_predictions}")
    print(f"  å®šä½æ­£ç¡®æ•°:      {correct_localizations} (IoUâ‰¥{iou_threshold})")
    print(f"  æ£€æµ‹æ­£ç¡®æ•°:      {correct_detections} (IoUâ‰¥{iou_threshold} ä¸”ç±»åˆ«å¯¹)")
    print(f"  ")
    print(f"  å®šä½å‡†ç¡®ç‡:      {localization_accuracy:.1%}  {'âœ…' if localization_accuracy >= 0.85 else 'âš ï¸'}")
    print(f"  (æ‰€æœ‰æ£€æµ‹æ¡†ä¸­ï¼Œä½ç½®æ­£ç¡®çš„æ¯”ä¾‹)")
    print(f"  ")
    print(f"  æ£€æµ‹å‡†ç¡®ç‡:      {detection_accuracy:.1%}  {'âœ…' if detection_accuracy >= 0.85 else 'âš ï¸'}")
    print(f"  (æ‰€æœ‰æ£€æµ‹æ¡†ä¸­ï¼Œä½ç½®+ç±»åˆ«éƒ½æ­£ç¡®çš„æ¯”ä¾‹ï¼Œâ‰ˆPrecision)")
    
    if localization_metrics['per_class']:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å„ç±»åˆ«å®šä½å‡†ç¡®ç‡:")
        print(f"{'='*60}")
        print(f"  {'ç±»åˆ«':<15} {'å®šä½å‡†ç¡®ç‡':<12} {'æ£€æµ‹å‡†ç¡®ç‡':<12} {'é¢„æµ‹æ•°':<10}")
        print(f"  {'-'*55}")
        for class_name, metrics in localization_metrics['per_class'].items():
            if metrics['predictions'] > 0:
                loc_status = 'âœ…' if metrics['localization_accuracy'] >= 0.85 else 'âš ï¸'
                det_status = 'âœ…' if metrics['detection_accuracy'] >= 0.85 else 'âš ï¸'
                print(f"  {class_name:<15} {metrics['localization_accuracy']:.1%} {loc_status}      "
                      f"{metrics['detection_accuracy']:.1%} {det_status}      {metrics['predictions']:<10}")
    
    print(f"{'='*60}\n")
    
    return localization_metrics


def compute_iou(box1, box2):
    """
    è®¡ç®—ä¸¤ä¸ªæ¡†çš„ IoU
    
    Args:
        box1: [x1, y1, x2, y2]
        box2: [x1, y1, x2, y2]
        
    Returns:
        float: IoU å€¼
    """
    # è®¡ç®—äº¤é›†
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    
    # è®¡ç®—å¹¶é›†
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = box1_area + box2_area - intersection
    
    # è®¡ç®— IoU
    iou = intersection / union if union > 0 else 0
    return iou


def different_thresholds(model, data_yaml, conf_thresholds=[0.15, 0.25, 0.35, 0.45]):
    """
    æµ‹è¯•ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼ä¸‹çš„ Recall å’Œ Precision
    ç”¨äºæ‰¾åˆ°æœ€ä½³çš„ç½®ä¿¡åº¦é˜ˆå€¼
    
    Args:
        model: YOLO æ¨¡å‹å®ä¾‹
        data_yaml: æ•°æ®é›†é…ç½®æ–‡ä»¶
        conf_thresholds: è¦æµ‹è¯•çš„ç½®ä¿¡åº¦é˜ˆå€¼åˆ—è¡¨
        
    Returns:
        list: ä¸åŒé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡ç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ æµ‹è¯•ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼")
    print(f"{'='*60}")
    print(f"ğŸ“ æ•°æ®é›†: {data_yaml}")
    print(f"ğŸ¯ æµ‹è¯•é˜ˆå€¼: {conf_thresholds}")
    print(f"{'='*60}\n")
    
    threshold_results = []
    
    for conf in conf_thresholds:
        print(f"â³ æµ‹è¯•é˜ˆå€¼: {conf:.2f}...")
        
        results = model.val(
            data=data_yaml,
            split='val',
            conf=conf,
            iou=EVAL_ARGS['iou'],
            device=EVAL_ARGS['device'],
            verbose=False,
            plots=False,
        )
        
        precision = float(results.box.mp)
        recall = float(results.box.mr)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        threshold_results.append({
            'conf_threshold': conf,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'mAP50': float(results.box.map50),
        })
        
        status = 'âœ…' if precision >= 0.85 or recall >= 0.85 else 'âš ï¸'
        print(f"   Precision: {precision:.1%}, Recall: {recall:.1%}, F1: {f1:.1%}  {status}\n")
    
    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    print(f"\n{'='*60}")
    print(f"ğŸ“Š é˜ˆå€¼æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"{'='*60}")
    print(f"  {'Conf':<8} {'Precision':<12} {'Recall':<10} {'F1':<10} {'mAP50':<10}")
    print(f"  {'-'*55}")
    
    for result in threshold_results:
        prec_status = 'âœ…' if result['precision'] >= 0.85 else '  '
        rec_status = 'âœ…' if result['recall'] >= 0.85 else '  '
        f1_status = 'âœ…' if result['f1_score'] >= 0.85 else '  '
        
        print(f"  {result['conf_threshold']:<8.2f} {result['precision']:.1%} {prec_status}   "
              f"{result['recall']:.1%} {rec_status}  {result['f1_score']:.1%} {f1_status}  "
              f"{result['mAP50']:.1%}")
    
    print(f"{'='*60}\n")
    
    # æ‰¾å‡ºæœ€ä½³é˜ˆå€¼
    best_for_precision = max(threshold_results, key=lambda x: x['precision'])
    best_for_recall = max(threshold_results, key=lambda x: x['recall'])
    best_for_f1 = max(threshold_results, key=lambda x: x['f1_score'])
    
    print(f"ğŸ’¡ æ¨èé˜ˆå€¼:")
    print(f"  - æœ€é«˜ Precision: conf={best_for_precision['conf_threshold']:.2f} (Precision={best_for_precision['precision']:.1%})")
    print(f"  - æœ€é«˜ Recall:    conf={best_for_recall['conf_threshold']:.2f} (Recall={best_for_recall['recall']:.1%})")
    print(f"  - æœ€é«˜ F1-Score:  conf={best_for_f1['conf_threshold']:.2f} (F1={best_for_f1['f1_score']:.1%})")
    print(f"{'='*60}\n")
    
    return threshold_results


def main():
    """ä¸»å‡½æ•°ï¼šåŠ è½½æ¨¡å‹å¹¶è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ YOLO æ¨¡å‹æŒ‡æ ‡åˆ†æ")
    print(f"{'='*60}")
    print(f"ğŸ“¦ æ¨¡å‹: {MODEL_PATH}")
    print(f"ğŸ“ æ•°æ®é›†: {DATA_YAML}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}/")
    print(f"{'='*60}\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”§ åŠ è½½æ¨¡å‹...")
    model = YOLO(MODEL_PATH)
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"   ç±»åˆ«: {model.names}\n")
    
    # ====== 1. è®¡ç®— Recall æŒ‡æ ‡ ======
    recall_metrics = calculate_recall_metrics(
        model, 
        DATA_YAML, 
        conf_threshold=EVAL_ARGS['conf']
    )
    
    # ====== 2. è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ ======
    classification_metrics = calculate_classification_accuracy(
        model, 
        DATA_YAML,
        conf_threshold=EVAL_ARGS['conf'],
        iou_threshold=EVAL_ARGS['iou']
    )
    
    # ====== 3. è®¡ç®—å®šä½å‡†ç¡®ç‡ ======
    localization_metrics = calculate_localization_accuracy(
        model,
        DATA_YAML,
        conf_threshold=EVAL_ARGS['conf'],
        iou_threshold=EVAL_ARGS['iou']
    )
    
    # ====== 4. æµ‹è¯•ä¸åŒé˜ˆå€¼ï¼ˆå¯é€‰ï¼‰======
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ æµ‹è¯•ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆæ‰¾åˆ° â‰¥85% çš„æœ€ä½³é…ç½®ï¼‰")
    print(f"{'='*60}")
    
    threshold_results = different_thresholds(
        model,
        DATA_YAML,
        conf_thresholds=[0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50]
    )
    
    # ====== ä¿å­˜ç»“æœ ======
    output_file = OUTPUT_DIR / f"metrics_analysis_{TIMESTAMP}.json"
    
    final_results = {
        'timestamp': TIMESTAMP,
        'model_path': str(MODEL_PATH),
        'data_yaml': str(DATA_YAML),
        'recall_metrics': recall_metrics,
        'classification_metrics': classification_metrics,
        'localization_metrics': localization_metrics,
        'threshold_test_results': threshold_results,
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å®Œæ•´ç»“æœä¿å­˜åˆ°: {output_file}")
    
    # ====== æœ€ç»ˆæ€»ç»“ ======
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æœ€ç»ˆæ€»ç»“")
    print(f"{'='*60}")
    print(f"\n1ï¸âƒ£  Recall (å¬å›ç‡):")
    print(f"   {recall_metrics['overall_recall']:.1%}  {'âœ… è¾¾æ ‡ï¼' if recall_metrics['overall_recall'] >= 0.85 else 'âš ï¸ æœªè¾¾ 85%'}")
    
    print(f"\n2ï¸âƒ£  Classification Accuracy (åˆ†ç±»å‡†ç¡®ç‡):")
    print(f"   {classification_metrics['classification_accuracy_approx']:.1%}  {'âœ… è¾¾æ ‡ï¼' if classification_metrics['classification_accuracy_approx'] >= 0.85 else 'âš ï¸ æœªè¾¾ 85%'}")
    
    print(f"\n3ï¸âƒ£  Localization Accuracy (å®šä½å‡†ç¡®ç‡):")
    print(f"   {localization_metrics['localization_accuracy']:.1%}  {'âœ… è¾¾æ ‡ï¼' if localization_metrics['localization_accuracy'] >= 0.85 else 'âš ï¸ æœªè¾¾ 85%'}")
    print(f"   (æ‰€æœ‰æ£€æµ‹æ¡†ä¸­ï¼Œä½ç½®æ­£ç¡® IoUâ‰¥0.5 çš„æ¯”ä¾‹)")
    
    print(f"\n4ï¸âƒ£  F1-Score (ç»¼åˆæŒ‡æ ‡):")
    print(f"   {recall_metrics['f1_score']:.1%}  {'âœ… è¾¾æ ‡ï¼' if recall_metrics['f1_score'] >= 0.85 else 'âš ï¸ æœªè¾¾ 85%'}")
    
    # å»ºè®®
    print(f"\n{'='*60}")
    print(f"ğŸ’¡ å»ºè®®:")
    print(f"{'='*60}")
    
    if localization_metrics['localization_accuracy'] >= 0.85:
        print(f"âœ… å®šä½å‡†ç¡®ç‡å·²è¾¾ 85%ï¼Œè¯´æ˜æ¨¡å‹å®šä½èƒ½åŠ›å¼ºï¼")
        print(f"   å¯ä»¥åœ¨æŠ¥å‘Šä¸­å¼ºè°ƒï¼š'{localization_metrics['localization_accuracy']:.1%} çš„æ£€æµ‹æ¡†ä½ç½®å‡†ç¡®ï¼ˆIoUâ‰¥0.5ï¼‰'")
    if classification_metrics['classification_accuracy_approx'] >= 0.85:
        print(f"âœ… Precision (â‰ˆåˆ†ç±»å‡†ç¡®ç‡) å·²è¾¾ 85%ï¼Œå¯ä»¥ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼")
    elif recall_metrics['f1_score'] >= 0.85:
        print(f"âœ… F1-Score å·²è¾¾ 85%ï¼Œå¯ä»¥ä½œä¸ºç»¼åˆæŒ‡æ ‡ï¼")
    elif recall_metrics['overall_recall'] >= 0.85:
        print(f"âœ… Recall å·²è¾¾ 85%ï¼Œå¦‚æœä»»åŠ¡é‡è§†\"ä¸æ¼æ£€\"ï¼Œå¯ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼")
    else:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æŒ‡æ ‡è¾¾æ ‡
        if localization_metrics['localization_accuracy'] >= 0.85:
            print(f"   è™½ç„¶å®šä½å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†åˆ†ç±»æ€§èƒ½è¿˜éœ€æå‡")
        else:
            print(f"âš ï¸ æ‰€æœ‰æŒ‡æ ‡éƒ½æœªè¾¾ 85%ï¼Œå»ºè®®ï¼š")
            print(f"   1. è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå‚è€ƒä¸Šé¢çš„é˜ˆå€¼æµ‹è¯•ç»“æœï¼‰")
            print(f"   2. ä¼˜åŒ–æ ‡æ³¨è´¨é‡")
            print(f"   3. å¢åŠ è®­ç»ƒæ•°æ®")
    
    print(f"{'='*60}\n")
    print(f"ğŸ‰ åˆ†æå®Œæˆï¼")
    print(f"{'='*60}\n")


if __name__ == '__main__':
    main()
