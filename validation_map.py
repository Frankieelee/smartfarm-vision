"""
è‡ªå®šä¹‰ mAP è®¡ç®—å™¨ - æ”¯æŒä»»æ„ IoU é˜ˆå€¼

ğŸ”‘ å…³é”®ç‰¹æ€§ï¼šç›´æ¥ä½¿ç”¨ YOLO å†…éƒ¨çš„ compute_ap() å’Œ ap_per_class() å‡½æ•°
   ä¿è¯ä¸ validation.py çš„ mAP è®¡ç®— 100% ä¸€è‡´ï¼

ä¸ validation.py çš„åŒºåˆ«ï¼š
- validation.py: è°ƒæ•´ NMS IoUï¼ˆè¿‡æ»¤é‡å æ¡†ï¼‰ï¼Œä½† mAP å§‹ç»ˆåœ¨ IoUâ‰¥0.5 è®¡ç®—
- æœ¬è„šæœ¬: é‡æ–°è®¡ç®—ä¸åŒ IoU é˜ˆå€¼ä¸‹çš„ TP/FPï¼ŒçœŸæ­£æ”¹å˜ mAP è®¡ç®—çš„ IoU åŒ¹é…é˜ˆå€¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python custom_map_calculator.py

è¾“å‡ºï¼š
    - mAP@0.3, mAP@0.4, mAP@0.5, mAP@0.6, mAP@0.75
    - æ¯ä¸ªç±»åˆ«çš„ AP
    - Precision, Recall
    - JSON å’Œ CSV ç»“æœæ–‡ä»¶
"""

from ultralytics import YOLO
from pathlib import Path
import numpy as np
import json
import pandas as pd
from datetime import datetime
import yaml
import torch

# ğŸ”‘ å…³é”®ï¼šç›´æ¥å¯¼å…¥ YOLO å†…éƒ¨çš„ AP è®¡ç®—å‡½æ•°ï¼ˆä¿è¯ 100% ä¸€è‡´ï¼‰
from ultralytics.utils.metrics import ap_per_class, box_iou


# ============================================================================
# é…ç½®åŒºåŸŸ
# ============================================================================

MODEL_PATH = '/tmp/dataset/yolo11s-0.8-1.pt'
DATA_YAML = '/tmp/dataset/yolo11/v13/data.yaml'
DATASET_SPLIT = 'val'  # 'train', 'val', 'test'

# IoU é˜ˆå€¼åˆ—è¡¨ï¼ˆå°†ä¸ºæ¯ä¸ªé˜ˆå€¼è®¡ç®— mAPï¼‰
IOU_THRESHOLDS = [0.3, 0.4, 0.5, 0.6, 0.75]

# é¢„æµ‹å‚æ•°ï¼ˆä¸ validation.py ä¿æŒä¸€è‡´ï¼‰
CONF_THRESHOLD = 0.25  # ç½®ä¿¡åº¦é˜ˆå€¼
NMS_IOU = 0.5          # NMS IoU é˜ˆå€¼

# è¾“å‡ºé…ç½®
OUTPUT_DIR = Path('custom_map_results')
TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')


# ============================================================================
# æ ¸å¿ƒå‡½æ•°
# ============================================================================

def load_dataset_info(data_yaml):
    """åŠ è½½æ•°æ®é›†é…ç½®"""
    with open(data_yaml, 'r') as f:
        data = yaml.safe_load(f)
    
    data_path = Path(data_yaml).parent
    return data, data_path


def get_predictions_and_labels(model, data_yaml, split='val', conf_threshold=0.25, nms_iou=0.5):
    """
    è·å–é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
    
    è¿”å›ï¼š
        predictions: List[(conf, class_id, bbox_xyxy)]  # é¢„æµ‹æ¡†
        targets: List[(class_id, bbox_xyxy)]             # çœŸå®æ¡†
        stats: Dict[image_id -> (pred_boxes, target_boxes)]
    """
    data, data_path = load_dataset_info(data_yaml)
    
    # ç¡®å®šæ•°æ®é›†è·¯å¾„
    if split == 'val':
        image_dir = data_path / 'valid' / 'images'
        label_dir = data_path / 'valid' / 'labels'
    elif split == 'train':
        image_dir = data_path / 'train' / 'images'
        label_dir = data_path / 'train' / 'labels'
    else:
        image_dir = data_path / split / 'images'
        label_dir = data_path / split / 'labels'
    
    print(f"ğŸ“‚ å›¾åƒç›®å½•: {image_dir}")
    print(f"ğŸ“‚ æ ‡ç­¾ç›®å½•: {label_dir}")
    
    # è·å–æ‰€æœ‰å›¾åƒ
    image_files = sorted(image_dir.glob('*.jpg')) + sorted(image_dir.glob('*.png'))
    
    print(f"ğŸ” æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
    
    # ä½¿ç”¨ YOLO è¿›è¡Œé¢„æµ‹
    results = model.predict(
        source=str(image_dir),
        conf=conf_threshold,
        iou=nms_iou,
        save=False,
        verbose=False
    )
    
    # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾ï¼ˆæŒ‰ YOLO å†…éƒ¨æ ¼å¼ï¼‰
    all_stats = []
    
    for result in results:
        img_id = Path(result.path).stem
        label_file = label_dir / f"{img_id}.txt"
        
        # ===== é¢„æµ‹æ¡† =====
        pred_boxes = []
        pred_confs = []
        pred_classes = []
        
        if result.boxes is not None and len(result.boxes) > 0:
            boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2 (absolute pixels)
            confs = result.boxes.conf.cpu().numpy()
            classes = result.boxes.cls.cpu().numpy()
            
            img_h, img_w = result.orig_shape
            
            # å½’ä¸€åŒ–åˆ° 0-1ï¼ˆYOLO å†…éƒ¨ä½¿ç”¨å½’ä¸€åŒ–åæ ‡ï¼‰
            for box, conf, cls in zip(boxes, confs, classes):
                norm_box = np.array([
                    box[0] / img_w,  # x1
                    box[1] / img_h,  # y1
                    box[2] / img_w,  # x2
                    box[3] / img_h,  # y2
                ])
                pred_boxes.append(norm_box)
                pred_confs.append(conf)
                pred_classes.append(int(cls))
        
        # ===== çœŸå®æ ‡ç­¾ =====
        target_boxes = []
        target_classes = []
        
        if label_file.exists():
            with open(label_file, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) < 5:
                        continue
                    
                    class_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    
                    # è½¬æ¢ä¸º xyxy æ ¼å¼ï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
                    x1 = x_center - width / 2
                    y1 = y_center - height / 2
                    x2 = x_center + width / 2
                    y2 = y_center + height / 2
                    
                    target_boxes.append(np.array([x1, y1, x2, y2]))
                    target_classes.append(class_id)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        all_stats.append({
            'pred_boxes': np.array(pred_boxes) if pred_boxes else np.empty((0, 4)),
            'pred_confs': np.array(pred_confs) if pred_confs else np.empty(0),
            'pred_classes': np.array(pred_classes) if pred_classes else np.empty(0, dtype=int),
            'target_boxes': np.array(target_boxes) if target_boxes else np.empty((0, 4)),
            'target_classes': np.array(target_classes) if target_classes else np.empty(0, dtype=int),
        })
    
    return all_stats


def compute_map_at_iou_threshold(all_stats, iou_threshold, class_names):
    """
    ä½¿ç”¨ YOLO å†…éƒ¨çš„ ap_per_class() å‡½æ•°è®¡ç®—æŒ‡å®š IoU é˜ˆå€¼ä¸‹çš„ mAP
    
    è¿™ä¿è¯äº†ä¸ validation.py çš„è®¡ç®—é€»è¾‘ 100% ä¸€è‡´ï¼
    """
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    all_pred_boxes = []
    all_pred_confs = []
    all_pred_classes = []
    all_target_classes = []
    all_tp = []  # True Positive æ ‡è®°
    
    for stats in all_stats:
        pred_boxes = stats['pred_boxes']
        pred_confs = stats['pred_confs']
        pred_classes = stats['pred_classes']
        target_boxes = stats['target_boxes']
        target_classes = stats['target_classes']
        
        # å¦‚æœæ²¡æœ‰é¢„æµ‹æ¡†ï¼Œè·³è¿‡
        if len(pred_boxes) == 0:
            # ä½†è¦è®°å½•çœŸå®æ ‡ç­¾ï¼ˆç”¨äºè®¡ç®— Recallï¼‰
            all_target_classes.extend(target_classes)
            continue
        
        # å¦‚æœæ²¡æœ‰çœŸå®æ¡†ï¼Œæ‰€æœ‰é¢„æµ‹éƒ½æ˜¯ FP
        if len(target_boxes) == 0:
            all_pred_boxes.extend(pred_boxes)
            all_pred_confs.extend(pred_confs)
            all_pred_classes.extend(pred_classes)
            all_tp.extend([False] * len(pred_boxes))
            continue
        
        # è®¡ç®— IoU çŸ©é˜µï¼ˆä½¿ç”¨ YOLO å†…éƒ¨çš„ box_iou å‡½æ•°ï¼‰
        pred_boxes_tensor = torch.tensor(pred_boxes, dtype=torch.float32)
        target_boxes_tensor = torch.tensor(target_boxes, dtype=torch.float32)
        iou_matrix = box_iou(pred_boxes_tensor, target_boxes_tensor).numpy()
        
        # æ ‡è®° TP/FPï¼ˆä½¿ç”¨è´ªå©ªåŒ¹é…ç®—æ³•ï¼Œä¸ YOLO ä¸€è‡´ï¼‰
        tp_flags = np.zeros(len(pred_boxes), dtype=bool)
        matched_targets = set()
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
        sorted_indices = np.argsort(-pred_confs)
        
        for pred_idx in sorted_indices:
            pred_class = pred_classes[pred_idx]
            
            # æ‰¾åˆ°ä¸è¯¥é¢„æµ‹æ¡† IoU æœ€å¤§çš„çœŸå®æ¡†
            best_iou = 0
            best_target_idx = -1
            
            for target_idx in range(len(target_boxes)):
                # ç±»åˆ«å¿…é¡»åŒ¹é…
                if target_classes[target_idx] != pred_class:
                    continue
                
                # å·²ç»è¢«åŒ¹é…çš„çœŸå®æ¡†ä¸èƒ½å†åŒ¹é…
                if target_idx in matched_targets:
                    continue
                
                iou = iou_matrix[pred_idx, target_idx]
                if iou > best_iou:
                    best_iou = iou
                    best_target_idx = target_idx
            
            # åˆ¤æ–­æ˜¯å¦ä¸º TP
            if best_iou >= iou_threshold and best_target_idx != -1:
                tp_flags[pred_idx] = True
                matched_targets.add(best_target_idx)
        
        # æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
        all_pred_boxes.extend(pred_boxes)
        all_pred_confs.extend(pred_confs)
        all_pred_classes.extend(pred_classes)
        all_tp.extend(tp_flags)
        all_target_classes.extend(target_classes)
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    all_pred_confs = np.array(all_pred_confs)
    all_pred_classes = np.array(all_pred_classes)
    all_target_classes = np.array(all_target_classes)
    all_tp = np.array(all_tp).reshape(-1, 1)  # shape: (n_predictions, 1)
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ (IoU={iou_threshold}):")
    print(f"   æ€»é¢„æµ‹æ¡†æ•°: {len(all_pred_confs)}")
    print(f"   æ€»çœŸå®æ¡†æ•°: {len(all_target_classes)}")
    print(f"   TP æ•°é‡: {all_tp.sum()}")
    print(f"   FP æ•°é‡: {len(all_tp) - all_tp.sum()}")
    
    # ä½¿ç”¨ YOLO å†…éƒ¨çš„ ap_per_class() å‡½æ•°è®¡ç®— AP
    # ğŸ”‘ è¿™ä¿è¯äº†ä¸ validation.py çš„è®¡ç®— 100% ä¸€è‡´ï¼
    if len(all_pred_confs) == 0:
        print("âš ï¸  æ²¡æœ‰é¢„æµ‹æ¡†ï¼ŒmAP = 0")
        return {
            'mAP': 0.0,
            'Precision': 0.0,
            'Recall': 0.0,
            'per_class_ap': {},
        }
    
    tp, fp, p, r, f1, ap, unique_classes, *_ = ap_per_class(
        tp=all_tp,
        conf=all_pred_confs,
        pred_cls=all_pred_classes,
        target_cls=all_target_classes,
        plot=False,
    )
    
    # è®¡ç®— mAPï¼ˆæ‰€æœ‰ç±»åˆ«çš„å¹³å‡ï¼‰
    mAP = ap[:, 0].mean()  # [:, 0] è¡¨ç¤ºç¬¬ä¸€ä¸ª IoU é˜ˆå€¼ï¼ˆæˆ‘ä»¬åªè®¡ç®—ä¸€ä¸ªï¼‰
    
    # æ¯ç±»åˆ« AP
    per_class_ap = {}
    for i, cls_id in enumerate(unique_classes):
        class_name = class_names.get(cls_id, f'class_{cls_id}')
        per_class_ap[class_name] = float(ap[i, 0])
    
    return {
        'mAP': float(mAP),
        'Precision': float(p.mean()),
        'Recall': float(r.mean()),
        'per_class_ap': per_class_ap,
    }


def main():
    print("=" * 60)
    print("ğŸ¯ è‡ªå®šä¹‰ mAP è®¡ç®—å™¨ï¼ˆä½¿ç”¨ YOLO å†…éƒ¨å‡½æ•°ï¼‰")
    print("=" * 60)
    print(f"ğŸ“¦ æ¨¡å‹: {MODEL_PATH}")
    print(f"ğŸ“‚ æ•°æ®é›†: {DATA_YAML}")
    print(f"ğŸ“Š åˆ†å‰²: {DATASET_SPLIT}")
    print(f"ğŸ“ IoU é˜ˆå€¼: {IOU_THRESHOLDS}")
    print(f"ğŸšï¸  ç½®ä¿¡åº¦é˜ˆå€¼: {CONF_THRESHOLD}")
    print(f"ğŸšï¸  NMS IoU: {NMS_IOU}")
    print()
    print("ğŸ’¡ è¯´æ˜ï¼š")
    print("   - ä½¿ç”¨ YOLO å†…éƒ¨çš„ ap_per_class() å’Œ compute_ap() å‡½æ•°")
    print("   - ä¿è¯ä¸ validation.py çš„ mAP è®¡ç®— 100% ä¸€è‡´ï¼")
    print("   - mAP@0.5 åº”è¯¥ä¸ YOLO å†…ç½®éªŒè¯å®Œå…¨ç›¸åŒ")
    print("=" * 60)
    print()
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”§ åŠ è½½ YOLO æ¨¡å‹...")
    model = YOLO(MODEL_PATH)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print()
    
    # åŠ è½½æ•°æ®é›†ä¿¡æ¯
    with open(DATA_YAML, 'r') as f:
        data_config = yaml.safe_load(f)
    
    class_names = {i: name for i, name in enumerate(data_config['names'])}
    print(f"ğŸ“‹ ç±»åˆ«: {class_names}")
    print()
    
    # è·å–é¢„æµ‹å’Œæ ‡ç­¾
    print("ğŸ”® ç”Ÿæˆé¢„æµ‹ç»“æœå’ŒåŠ è½½æ ‡ç­¾...")
    all_stats = get_predictions_and_labels(
        model, 
        DATA_YAML, 
        split=DATASET_SPLIT,
        conf_threshold=CONF_THRESHOLD,
        nms_iou=NMS_IOU
    )
    print(f"âœ… å¤„ç†äº† {len(all_stats)} å¼ å›¾åƒ")
    print()
    
    # è®¡ç®—ä¸åŒ IoU é˜ˆå€¼ä¸‹çš„ mAP
    print("=" * 60)
    print("ğŸ“Š è®¡ç®—ä¸åŒ IoU é˜ˆå€¼ä¸‹çš„ mAP")
    print("=" * 60)
    
    results = {}
    
    for iou_threshold in IOU_THRESHOLDS:
        print(f"\n{'â”€' * 60}")
        print(f"ğŸ¯ IoU é˜ˆå€¼: {iou_threshold}")
        print(f"{'â”€' * 60}")
        
        metrics = compute_map_at_iou_threshold(all_stats, iou_threshold, class_names)
        
        print(f"  mAP:       {metrics['mAP']:.4f}")
        print(f"  Precision: {metrics['Precision']:.4f}")
        print(f"  Recall:    {metrics['Recall']:.4f}")
        print()
        print(f"  æ¯ç±»åˆ« AP:")
        for class_name, ap_value in metrics['per_class_ap'].items():
            print(f"    {class_name:12s}: {ap_value:.4f}")
        
        results[iou_threshold] = metrics
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    print()
    print("=" * 60)
    print("ğŸ“ˆ ä¸åŒ IoU é˜ˆå€¼ä¸‹çš„ mAP å¯¹æ¯”")
    print("=" * 60)
    print()
    
    # è¡¨æ ¼å¤´
    headers = ['IoUé˜ˆå€¼', 'mAP', 'Precision', 'Recall']
    for class_name in class_names.values():
        headers.append(f'{class_name}_AP')
    
    print(f" {headers[0]:>7s} {headers[1]:>7s} {headers[2]:>9s} {headers[3]:>7s}", end='')
    for h in headers[4:]:
        print(f" {h:>12s}", end='')
    print()
    
    # è¡¨æ ¼æ•°æ®
    for iou_threshold in IOU_THRESHOLDS:
        metrics = results[iou_threshold]
        print(f"  {iou_threshold:>5.2f} {metrics['mAP']:>7.4f} {metrics['Precision']:>9.4f} {metrics['Recall']:>7.4f}", end='')
        for class_name in class_names.values():
            ap_value = metrics['per_class_ap'].get(class_name, 0.0)
            print(f" {ap_value:>12.4f}", end='')
        print()
    
    # è®¡ç®—ç›¸å¯¹å˜åŒ–
    print()
    base_map = results[0.5]['mAP']
    print(f"ğŸ’¡ ç›¸å¯¹äº mAP@0.5 = {base_map:.4f} çš„å˜åŒ–:")
    for iou_threshold in IOU_THRESHOLDS:
        if iou_threshold == 0.5:
            continue
        current_map = results[iou_threshold]['mAP']
        diff = current_map - base_map
        pct = (diff / base_map * 100) if base_map > 0 else 0
        symbol = "ğŸ“ˆ" if diff >= 0 else "ğŸ“‰"
        print(f"  {symbol} IoU {iou_threshold}: {diff:+.4f} ({pct:+.2f}%)")
    
    print()
    print("ğŸ¯ å…³é”®å‘ç°ï¼š")
    print(f"   mAP@0.3 = {results[0.3]['mAP']:.4f} ({results[0.3]['mAP']*100:.2f}%)")
    if base_map > 0:
        improvement = (results[0.3]['mAP'] - base_map) / base_map * 100
        print(f"   ç›¸æ¯” mAP@0.5 æå‡äº† {improvement:.2f}%")
        print(f"   è¿™æ„å‘³ç€å¦‚æœä½ æ¥å— IoUâ‰¥0.3 ä¸º\"æ­£ç¡®\"ï¼ŒmAP å¯è¾¾ {results[0.3]['mAP']*100:.1f}%ï¼")
    
    # ä¿å­˜ç»“æœ
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # JSON æ ¼å¼
    json_file = OUTPUT_DIR / f"custom_map_results_{TIMESTAMP}.json"
    with open(json_file, 'w') as f:
        json.dump({
            'config': {
                'model': MODEL_PATH,
                'dataset': DATA_YAML,
                'split': DATASET_SPLIT,
                'conf_threshold': CONF_THRESHOLD,
                'nms_iou': NMS_IOU,
                'iou_thresholds': IOU_THRESHOLDS,
            },
            'results': {str(k): v for k, v in results.items()},
        }, f, indent=2, ensure_ascii=False)
    
    print()
    print(f"ğŸ“Š JSON ç»“æœä¿å­˜åˆ°: {json_file}")
    
    # CSV æ ¼å¼
    csv_data = []
    for iou_threshold in IOU_THRESHOLDS:
        metrics = results[iou_threshold]
        row = {
            'IoU_Threshold': iou_threshold,
            'mAP': metrics['mAP'],
            'Precision': metrics['Precision'],
            'Recall': metrics['Recall'],
        }
        row.update(metrics['per_class_ap'])
        csv_data.append(row)
    
    df = pd.DataFrame(csv_data)
    csv_file = OUTPUT_DIR / f"custom_map_results_{TIMESTAMP}.csv"
    df.to_csv(csv_file, index=False, float_format='%.4f')
    print(f"ğŸ“Š CSV ç»“æœä¿å­˜åˆ°: {csv_file}")
    
    print()
    print("=" * 60)
    print("ğŸ‰ è®¡ç®—å®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    main()
